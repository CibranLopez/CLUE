{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6216fb5c-6b52-4fcf-a34c-3830ec5b0b99",
   "metadata": {},
   "source": [
    "# Load libraries and set device up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0800a3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:09.202615Z",
     "start_time": "2025-02-19T16:47:08.407766Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy    as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import libraries.model   as clm\n",
    "import libraries.dataset as cld\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b4fab8-6fed-40b9-92fa-9d69bf7a0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bandgap'\n",
    "folder = 'MP-bandgap'\n",
    "\n",
    "target_folder = f'models/{folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dac005723c01453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:35.665241Z",
     "start_time": "2025-02-19T16:47:35.661963Z"
    }
   },
   "outputs": [],
   "source": [
    "files_names = {\n",
    "    'dataset':            f'{target_folder}/dataset.pt',\n",
    "    'train_dataset_std':  f'{target_folder}/train_dataset_std.pt',\n",
    "    'val_dataset_std':    f'{target_folder}/val_dataset_std.pt',\n",
    "    'test_dataset_std':   f'{target_folder}/test_dataset_std.pt',\n",
    "    'r_dataset_std':      f'{target_folder}/ref_dataset_std.pt',\n",
    "    'std_parameters':     f'{target_folder}/standardized_parameters.json',\n",
    "    'dataset_parameters': f'{target_folder}/dataset_parameters.json',\n",
    "    'uncertainty_data':   f'{target_folder}/uncertainty_data.json',\n",
    "    'model':              f'{target_folder}/model.pt',\n",
    "    'model_parameters':   f'{target_folder}/model_parameters.json'\n",
    "}\n",
    "cld.save_json(files_names, f'{target_folder}/files_names.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb787e0-c325-4e25-815a-c35a9838a5c3",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd238bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:09.832021Z",
     "start_time": "2025-02-19T16:47:09.828977Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs =      300\n",
    "batch_size =    128\n",
    "learning_rate = 0.001\n",
    "dropout =       0.1\n",
    "patience =      n_epochs\n",
    "delta =         2\n",
    "train_ratio =   0.8\n",
    "test_ratio =    0.1  # val_ratio = 1 - train_ratio - test_ratio\n",
    "\n",
    "model_parameters = {\n",
    "    'n_epochs':      n_epochs,\n",
    "    'batch_size':    batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'dropout':       dropout,\n",
    "    'patience':      patience,\n",
    "    'delta':         delta,\n",
    "    'train_ratio':   train_ratio,\n",
    "    'test_ratio':    test_ratio\n",
    "}\n",
    "cld.save_json(model_parameters, files_names['model_parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df509c7",
   "metadata": {},
   "source": [
    "# Generate or load graph database for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35c84da-b5f8-4185-88a9-09229e77081d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:53:52.564297Z",
     "start_time": "2025-02-19T16:53:34.266107Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try loading the training datasets directly, else generate them\n",
    "try:\n",
    "    train_dataset, val_dataset, test_dataset, standardized_parameters = cld.load_datasets(files_names)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    if not os.path.exists(files_names['dataset']):\n",
    "        # Generate data\n",
    "        cld.generate_dataset('datasets/bandgap-MChX',\n",
    "                             targets=['bandgap'],\n",
    "                             data_folder=target_folder)\n",
    "\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(files_names['dataset_name'], weights_only=False)\n",
    "\n",
    "    # Load dataset parameters\n",
    "    dataset_parameters = cld.load_json(files_names['dataset_parameters'])\n",
    "\n",
    "    # Split datasets\n",
    "    train_dataset, val_dataset, test_dataset = cld.split_dataset(train_ratio, test_ratio, dataset)\n",
    "    del dataset  # Free up CUDA memory\n",
    "\n",
    "    # Standardize train dataset\n",
    "    train_dataset_std, standardized_parameters = cld.standardize_dataset(train_dataset)\n",
    "    del train_dataset\n",
    "\n",
    "    # Standardize test and validation datasets with train parameters\n",
    "    val_dataset_std  = cld.standardize_dataset_from_keys(val_dataset,  standardized_parameters)\n",
    "    test_dataset_std = cld.standardize_dataset_from_keys(test_dataset, standardized_parameters)\n",
    "    del val_dataset, test_dataset\n",
    "\n",
    "    # Save datasets\n",
    "    cld.save_datasets(train_dataset_std, val_dataset_std, test_dataset_std, files_names)\n",
    "\n",
    "    # Save standardized parameters\n",
    "    cld.save_json(standardized_parameters, files_names['std_parameters'])\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = np.array(standardized_parameters['target_std']) / standardized_parameters['scale']\n",
    "targe_mean    = standardized_parameters['target_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafd2c5319d53e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51293539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:54:35.676746Z",
     "start_time": "2025-02-19T16:54:35.671843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset_std, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset_std,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset_std,  batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0786d7",
   "metadata": {},
   "source": [
    "# Generate Graph Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dbbc6-bbae-4dfb-b008-3fead983ed40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:54:43.253628Z",
     "start_time": "2025-02-19T16:54:43.094332Z"
    }
   },
   "outputs": [],
   "source": [
    "model = clm.load_model(n_node_features, dropout, device, model_name=files_names['model'], mode='train')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d29bf-8c2d-4555-8a38-5681a908e770",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f4ee1-312f-4d8e-b000-01158351d516",
   "metadata": {},
   "source": [
    "Define training optimized and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37834c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:55:37.192535Z",
     "start_time": "2025-02-19T16:55:37.188870Z"
    }
   },
   "outputs": [],
   "source": [
    "# MSELoss is by default defined as the mean within the batch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = clm.EarlyStopping(patience=patience, delta=delta, model_name=files_names['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c53b48d2759cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses   = []\n",
    "for epoch in np.arange(0, n_epochs):\n",
    "    train_loss, train_predictions, train_ground_truths = clm.train(model, criterion, train_loader,\n",
    "                                                                   target_factor,\n",
    "                                                                   standardized_parameters['target_mean'],\n",
    "                                                                   optimizer)\n",
    "    val_loss,   val_predictions,   val_ground_truths   =  clm.test(model, criterion, val_loader,\n",
    "                                                                   target_factor,\n",
    "                                                                   standardized_parameters['target_mean'])\n",
    "\n",
    "    # Convert to original units\n",
    "    train_loss = np.sum(np.sqrt(train_loss) * target_factor)\n",
    "    val_loss   = np.sum(np.sqrt(val_loss)   * target_factor)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        cld.parity_plot(train=np.array([train_ground_truths, train_predictions]),\n",
    "                        validation=np.array([val_ground_truths, val_predictions]))\n",
    "    \n",
    "    # Append losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Check early stopping criteria\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train MAE: {train_loss:.4f}, Val MAE: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544042a-9d26-4aae-9946-efd6cb73359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:05:55.648166Z",
     "start_time": "2025-02-19T17:05:55.602515Z"
    }
   },
   "outputs": [],
   "source": [
    "cld.losses_plot(train_losses=train_losses,\n",
    "                val_losses=val_losses,\n",
    "                to_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215d1b6-c1a9-40c7-b77d-7314d8471cac",
   "metadata": {},
   "source": [
    "# Check test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f13a3-130d-4d7f-87e4-4afe14356414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:06:00.879092Z",
     "start_time": "2025-02-19T17:06:00.868367Z"
    }
   },
   "outputs": [],
   "source": [
    "model = clm.load_model(n_node_features, dropout, device, model_name=files_names['model'], mode='eval')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f403ac3-5b52-4e25-8fbc-f3b7ca890f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:06:09.191104Z",
     "start_time": "2025-02-19T17:06:01.655459Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss, train_predictions, train_ground_truths = clm.test(model, criterion, train_loader,\n",
    "                                                              target_factor,\n",
    "                                                              standardized_parameters['target_mean'])\n",
    "val_loss,   val_predictions,   val_ground_truths   = clm.test(model, criterion, val_loader,\n",
    "                                                              target_factor,\n",
    "                                                              standardized_parameters['target_mean'])\n",
    "test_loss,  test_predictions,  test_ground_truths  = clm.test(model, criterion, test_loader,\n",
    "                                                              target_factor,\n",
    "                                                              standardized_parameters['target_mean'])\n",
    "\n",
    "# Pass to energy units (same as initial Fv)\n",
    "train_loss = np.sum(np.sqrt(train_loss) * target_factor)\n",
    "val_loss   = np.sum(np.sqrt(val_loss)   * target_factor)\n",
    "test_loss  = np.sum(np.sqrt(test_loss)  * target_factor)\n",
    "\n",
    "cld.parity_plot(train=np.array([train_ground_truths, train_predictions]),\n",
    "                validation=np.array([val_ground_truths, val_predictions]),\n",
    "                test=np.array([test_ground_truths, test_predictions]),\n",
    "                save_to=f'{target_folder}/{target}-GCNN-training.pdf')\n",
    "\n",
    "print(f'Train MAE: {train_loss:.4f}, Val MAE: {val_loss:.4f}, Test MAE: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaeafafe1b71bb",
   "metadata": {},
   "source": [
    "# Generate uncertainties and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630c0d4-1fc3-48ae-a6c0-51686f0038c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:07:12.135816Z",
     "start_time": "2025-02-19T17:07:03.109485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define reference dataset\n",
    "# This is not necessarily just the full dataset, but it can include any\n",
    "# additional validation not used as train, test or validation\n",
    "dataset = [data for dataset in [train_dataset_std, val_dataset_std, test_dataset_std] for data in dataset]\n",
    "\n",
    "# Save reference dataset\n",
    "torch.save(dataset, files_names['r_dataset_std'])\n",
    "\n",
    "# Generate data loader\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "loss, predictions, ground_truths = clm.test(model, criterion, loader,\n",
    "                                            target_factor,\n",
    "                                            standardized_parameters['target_mean'])\n",
    "\n",
    "# label: uncertainty = ground-truth - prediction\n",
    "uncertainties = ground_truths - predictions\n",
    "\n",
    "# Standardize for better interpolation\n",
    "uncert_mean  = uncertainties.mean()\n",
    "uncert_std   = uncertainties.std()\n",
    "uncert_scale = 1\n",
    "\n",
    "uncertainties_std = (uncertainties - uncert_mean) * uncert_scale / uncert_std\n",
    "\n",
    "# Check all labels are unique, otherwise you should rename them\n",
    "labels = [data.label for data in dataset]\n",
    "if len(labels) == len(set(labels)):\n",
    "    print('All labels are unique')\n",
    "else:\n",
    "    print('Error: some labels are duplicated')\n",
    "\n",
    "# Generate dictionary with uncertainty data\n",
    "uncertainty_values = {}\n",
    "for idx in range(len(dataset)):\n",
    "    uncertainty_values.update(\n",
    "        {dataset[idx].label: float(uncertainties_std[idx])}\n",
    "    )\n",
    "uncertainty_data = {\n",
    "    'uncertainty_values': uncertainty_values,\n",
    "    'uncert_mean':        uncert_mean,\n",
    "    'uncert_std':         uncert_std,\n",
    "    'uncert_scale':       uncert_scale\n",
    "}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "cld.save_json(uncertainty_data, files_names['uncertainty_data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
