{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6216fb5c-6b52-4fcf-a34c-3830ec5b0b99",
   "metadata": {},
   "source": [
    "# Load libraries and set device up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0800a3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:09.202615Z",
     "start_time": "2025-02-19T16:47:08.407766Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy    as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import libraries.model   as clm\n",
    "import libraries.dataset as cld\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Checking if pytorch can run in GPU, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b4fab8-6fed-40b9-92fa-9d69bf7a0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bandgap'\n",
    "folder = 'MP-bandgap'\n",
    "\n",
    "target_folder = f'models/{folder}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dac005723c01453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:35.665241Z",
     "start_time": "2025-02-19T16:47:35.661963Z"
    }
   },
   "outputs": [],
   "source": [
    "files_names = {\n",
    "    'dataset_name':      f'{target_folder}/dataset.pt',\n",
    "    'train_dt_std_name': f'{target_folder}/train_dataset_std.pt',\n",
    "    'val_dt_std_name':   f'{target_folder}/val_dataset_std.pt',\n",
    "    'test_dt_std_name':  f'{target_folder}/test_dataset_std.pt',\n",
    "    'std_param_name':    f'{target_folder}/standardized_parameters.json',\n",
    "    'dt_param_name':     f'{target_folder}/dataset_parameters.json',\n",
    "    'uncert_data_name':  f'{target_folder}/uncertainty_data.json',\n",
    "    'model_name':        f'{target_folder}/model.pt',\n",
    "    'model_param_name':  f'{target_folder}/model_parameters.json'\n",
    "}\n",
    "cld.save_json(files_names, f'{target_folder}/files_names.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb787e0-c325-4e25-815a-c35a9838a5c3",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd238bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:47:09.832021Z",
     "start_time": "2025-02-19T16:47:09.828977Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs =      300\n",
    "batch_size =    128\n",
    "learning_rate = 0.001\n",
    "dropout =       0.1\n",
    "patience =      n_epochs\n",
    "delta =         2\n",
    "train_ratio =   0.8\n",
    "test_ratio =    0.1  # val_ratio = 1 - train_ratio - test_ratio\n",
    "\n",
    "model_parameters = {\n",
    "    'n_epochs':      n_epochs,\n",
    "    'batch_size':    batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'dropout':       dropout,\n",
    "    'patience':      patience,\n",
    "    'delta':         delta,\n",
    "    'train_ratio':   train_ratio,\n",
    "    'test_ratio':    test_ratio\n",
    "}\n",
    "cld.save_json(model_parameters, files_names['model_param_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df509c7",
   "metadata": {},
   "source": [
    "# Generate or load graph database for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35c84da-b5f8-4185-88a9-09229e77081d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:53:52.564297Z",
     "start_time": "2025-02-19T16:53:34.266107Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try loading the training datasets directly, else generate them\n",
    "try:\n",
    "    train_dataset, val_dataset, test_dataset, standardized_parameters = cld.load_datasets(files_names)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    if not os.path.exists(files_names['dataset_name']):\n",
    "        # Generate data\n",
    "        cld.generate_dataset('datasets/bandgap-MChX',\n",
    "                             targets=['bandgap'],\n",
    "                             data_folder=target_folder)\n",
    "\n",
    "    # Load the raw dataset, with corresponding labels, and standardize it\n",
    "    dataset = torch.load(files_names['dataset_name'], weights_only=False)\n",
    "\n",
    "    # Load dataset parameters\n",
    "    dataset_parameters = cld.load_json(files_names['dt_param_name'])\n",
    "\n",
    "    # Determine the index for the desired property\n",
    "    target_idx = [idx for idx, t in enumerate(dataset_parameters['target']) if t == target][0]\n",
    "\n",
    "    # Select the target we want\n",
    "    for data in dataset:\n",
    "        data.y = torch.tensor([data.y[target_idx]], dtype=torch.float)\n",
    "\n",
    "    # Split datasets\n",
    "    train_dataset, val_dataset, test_dataset = cld.split_dataset(train_ratio, test_ratio, dataset)\n",
    "    del dataset  # Free up CUDA memory\n",
    "\n",
    "    # Standardize train dataset\n",
    "    train_dataset, standardized_parameters = cld.standardize_dataset(train_dataset)\n",
    "\n",
    "    # Standardize test and validation datasets with train parameters\n",
    "    val_dataset  = cld.standardize_dataset_from_keys(val_dataset,  standardized_parameters)\n",
    "    test_dataset = cld.standardize_dataset_from_keys(test_dataset, standardized_parameters)\n",
    "\n",
    "    # Save datasets\n",
    "    cld.save_datasets(train_dataset, val_dataset, test_dataset, files_names)\n",
    "\n",
    "    # Save standardized parameters\n",
    "    cld.save_json(standardized_parameters, files_names['std_param_name'])\n",
    "\n",
    "# Defining target factor\n",
    "target_factor = np.array(standardized_parameters['target_std']) / standardized_parameters['scale']\n",
    "targe_mean = standardized_parameters['target_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafd2c5319d53e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51293539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:54:35.676746Z",
     "start_time": "2025-02-19T16:54:35.671843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Determine number of node-level features in dataset, considering the t_step information\n",
    "n_node_features = train_dataset[0].num_node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0786d7",
   "metadata": {},
   "source": [
    "# Generate Graph Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b719261c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:54:43.253628Z",
     "start_time": "2025-02-19T16:54:43.094332Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for eGCNN:\n\tMissing key(s) in state_dict: \"node_conv1.lin_rel.weight\", \"node_conv1.lin_rel.bias\", \"node_conv1.lin_root.weight\", \"node_conv2.lin_rel.weight\", \"node_conv2.lin_rel.bias\", \"node_conv2.lin_root.weight\", \"node_conv3.lin_rel.weight\", \"node_conv3.lin_rel.bias\", \"node_conv3.lin_root.weight\", \"edge_linear_f1.weight\", \"edge_linear_f1.bias\", \"edge_linear_r1.weight\", \"edge_linear_r1.bias\", \"edge_linear_f2.weight\", \"edge_linear_f2.bias\", \"edge_linear_r2.weight\", \"edge_linear_r2.bias\", \"node_norm1.weight\", \"node_norm1.bias\", \"node_norm1.running_mean\", \"node_norm1.running_var\", \"edge_norm1.weight\", \"edge_norm1.bias\", \"edge_norm1.running_mean\", \"edge_norm1.running_var\". \n\tUnexpected key(s) in state_dict: \"conv1.lin_rel.weight\", \"conv1.lin_rel.bias\", \"conv1.lin_root.weight\", \"conv2.lin_rel.weight\", \"conv2.lin_rel.bias\", \"conv2.lin_root.weight\", \"lin1.weight\", \"lin1.bias\", \"lin2.weight\", \"lin2.bias\", \"lin.weight\", \"lin.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mclm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_node_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpurpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\n",
      "File \u001b[0;32m~/cibran/Work/UPC/CLUE/libraries/model.py:682\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(n_node_features, pdropout, device, model_name, purpose)\u001b[0m\n\u001b[1;32m    678\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name):\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# Load Graph Neural Network model\u001b[39;00m\n\u001b[0;32m--> 682\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m purpose \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    685\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/cibran/Work/UPC/CLUE/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for eGCNN:\n\tMissing key(s) in state_dict: \"node_conv1.lin_rel.weight\", \"node_conv1.lin_rel.bias\", \"node_conv1.lin_root.weight\", \"node_conv2.lin_rel.weight\", \"node_conv2.lin_rel.bias\", \"node_conv2.lin_root.weight\", \"node_conv3.lin_rel.weight\", \"node_conv3.lin_rel.bias\", \"node_conv3.lin_root.weight\", \"edge_linear_f1.weight\", \"edge_linear_f1.bias\", \"edge_linear_r1.weight\", \"edge_linear_r1.bias\", \"edge_linear_f2.weight\", \"edge_linear_f2.bias\", \"edge_linear_r2.weight\", \"edge_linear_r2.bias\", \"node_norm1.weight\", \"node_norm1.bias\", \"node_norm1.running_mean\", \"node_norm1.running_var\", \"edge_norm1.weight\", \"edge_norm1.bias\", \"edge_norm1.running_mean\", \"edge_norm1.running_var\". \n\tUnexpected key(s) in state_dict: \"conv1.lin_rel.weight\", \"conv1.lin_rel.bias\", \"conv1.lin_root.weight\", \"conv2.lin_rel.weight\", \"conv2.lin_rel.bias\", \"conv2.lin_root.weight\", \"lin1.weight\", \"lin1.bias\", \"lin2.weight\", \"lin2.bias\", \"lin.weight\", \"lin.bias\". "
     ]
    }
   ],
   "source": [
    "model = clm.load_model(n_node_features, dropout, device, model_name=files_names['model_name'], purpose='train')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d29bf-8c2d-4555-8a38-5681a908e770",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f4ee1-312f-4d8e-b000-01158351d516",
   "metadata": {},
   "source": [
    "Define training optimized and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37834c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T16:55:37.192535Z",
     "start_time": "2025-02-19T16:55:37.188870Z"
    }
   },
   "outputs": [],
   "source": [
    "# MSELoss is by default defined as the mean within the batch\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = clm.EarlyStopping(patience=patience, delta=delta, model_name=files_names['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c53b48d2759cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses   = []\n",
    "for epoch in np.arange(0, n_epochs):\n",
    "    train_loss, train_predictions, train_ground_truths = clm.train(model, criterion, train_loader,\n",
    "                                                                   target_factor,\n",
    "                                                                   standardized_parameters['target_mean'],\n",
    "                                                                   optimizer)\n",
    "    val_loss,   val_predictions,   val_ground_truths   =  clm.test(model, criterion, val_loader,\n",
    "                                                                   target_factor,\n",
    "                                                                   standardized_parameters['target_mean'])\n",
    "\n",
    "    # Convert to original units\n",
    "    train_loss = np.sum(np.sqrt(train_loss) * target_factor)\n",
    "    val_loss   = np.sum(np.sqrt(val_loss)   * target_factor)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        cld.parity_plot(train=np.array([train_ground_truths, train_predictions]),\n",
    "                        validation=np.array([val_ground_truths, val_predictions]))\n",
    "    \n",
    "    # Append losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Check early stopping criteria\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Train MAE: {train_loss:.4f}, Val MAE: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544042a-9d26-4aae-9946-efd6cb73359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:05:55.648166Z",
     "start_time": "2025-02-19T17:05:55.602515Z"
    }
   },
   "outputs": [],
   "source": [
    "cld.losses_plot(train_losses=train_losses,\n",
    "                val_losses=val_losses,\n",
    "                to_log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215d1b6-c1a9-40c7-b77d-7314d8471cac",
   "metadata": {},
   "source": [
    "# Check test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f13a3-130d-4d7f-87e4-4afe14356414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:06:00.879092Z",
     "start_time": "2025-02-19T17:06:00.868367Z"
    }
   },
   "outputs": [],
   "source": [
    "model = clm.load_model(n_node_features, dropout, device, model_name=files_names['model_name'], purpose='eval')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f403ac3-5b52-4e25-8fbc-f3b7ca890f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:06:09.191104Z",
     "start_time": "2025-02-19T17:06:01.655459Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss, train_predictions, train_ground_truths = clm.test(model, criterion, train_loader,\n",
    "                                                              target_factor,\n",
    "                                                              standardized_parameters['target_mean'])\n",
    "val_loss,   val_predictions,   val_ground_truths   =  clm.test(model, criterion, val_loader,\n",
    "                                                               target_factor,\n",
    "                                                               standardized_parameters['target_mean'])\n",
    "test_loss,  test_predictions,  test_ground_truths  =  clm.test(model, criterion, test_loader,\n",
    "                                                               target_factor,\n",
    "                                                               standardized_parameters['target_mean'])\n",
    "\n",
    "# Pass to energy units (same as initial Fv)\n",
    "train_loss = np.sum(np.sqrt(train_loss) * target_factor)\n",
    "val_loss   = np.sum(np.sqrt(val_loss)   * target_factor)\n",
    "test_loss  = np.sum(np.sqrt(test_loss)  * target_factor)\n",
    "\n",
    "cld.parity_plot(train=np.array([train_ground_truths, train_predictions]),\n",
    "                validation=np.array([val_ground_truths, val_predictions]),\n",
    "                test=np.array([test_ground_truths, test_predictions]),\n",
    "                save_to=f'{target_folder}/{target}-GCNN-training.pdf')\n",
    "\n",
    "print(f'Train MAE: {train_loss:.4f}, Val MAE: {val_loss:.4f}, Test MAE: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eaeafafe1b71bb",
   "metadata": {},
   "source": [
    "# Generate uncertainties and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630c0d4-1fc3-48ae-a6c0-51686f0038c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T17:07:12.135816Z",
     "start_time": "2025-02-19T17:07:03.109485Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [data for dataset in [train_dataset, val_dataset, test_dataset] for data in dataset]\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "loss, predictions, ground_truths = clm.test(model, criterion, loader,\n",
    "                                            target_factor,\n",
    "                                            standardized_parameters['target_mean'])\n",
    "\n",
    "# label: uncertainty = ground-truth - prediction\n",
    "uncertainties = ground_truths - predictions\n",
    "\n",
    "# Standardize for better interpolation\n",
    "uncert_mean  = uncertainties.mean()\n",
    "uncert_std   = uncertainties.std()\n",
    "\n",
    "uncertainties_std = (uncertainties - uncert_mean) * standardized_parameters['scale'] / uncert_std\n",
    "\n",
    "# Generate dictionary with uncertainty data\n",
    "uncertainty_values = {}\n",
    "for idx in range(len(dataset)):\n",
    "    uncertainty_values.update(\n",
    "        {dataset[idx].label: float(uncertainties_std[idx])}\n",
    "    )\n",
    "uncertainty_data = {\n",
    "    'uncertainty_values': uncertainty_values,\n",
    "    'uncert_mean':        uncert_mean,\n",
    "    'uncert_std':         uncert_std\n",
    "}\n",
    "\n",
    "# Dump the dictionary with numpy arrays to a JSON file\n",
    "cld.save_json(uncertainty_data, files_names['uncert_data_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
